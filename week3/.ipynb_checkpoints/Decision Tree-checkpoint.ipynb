{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cây quyết định\n",
    "**Cây quyết định (Decision Tree)** là một phương pháp học máy có giám sát không tham số được sử dụng để phân lớp và hồi quy.\n",
    "\n",
    "Mục đích của cây quyết định là tạo ra một mô hình dự đoán kết quả mục tiêu bằng cách học các luật quyết định đơn giản được suy diễn ra từ các đặc trưng dữ liệu.\n",
    "\n",
    "Mỗi tập luật định nghĩa ra một giả thuyết, có thể được biểu diễn bằng một cây quyết định với đường đi xuôi từ gốc đến lá cho ta một luật quyết định. Nút gốc và mỗi nút trên cây là một thuộc tính/ điều kiện kiểm tra, các nhánh đi xuống từ nút ứng với các giá trị có thể của thuộc tính/điều kiện này. Nhãn của các mẫu phù hợp là các nút lá.\n",
    "\n",
    "\n",
    "Hình dưới đây minh họa một cây quyết định của dữ liệu **Titanic** dự đoán khả năng sống sót khi tàu chìm.\n",
    "<img src=\"titanic.png\" style=\"text-align:center; max-height:400px\">\n",
    "\n",
    "**Bài tập:** Mô tả tập luật của cây quyết định trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trả lời**: *Điền đáp án vào đây!*\n",
    "\n",
    "if (gender = male) and (age > 10) then died\n",
    "\n",
    "if (gender = male) and (age <= 10) and (number of siblings > 2) then died\n",
    "\n",
    "if (gender = male) and (age <= 10) and (number of siblings <= 2) then survived\n",
    "\n",
    "if (gender = female) then survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mô hình cây quyết định trong Scikit-learn\n",
    "Trong `Scikit-learn`, mô hình cây quyết định được cài đặt trong gói `tree` với `DecisionTreeClassifier`.\n",
    "\n",
    "**Bài tập:** Import dữ liệu và mô hình cây quyết định từ `Scikit-learn`, sau đó huấn luyện và biểu diễn mô hình thu được sau khi huấn luyện.\n",
    "\n",
    "*Gợi ý:* Sử dụng kiến thức từ bài thực hành trước với mô hình `SVM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f9f6154b3a4de4a8b52fa30ede2a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Label</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Label(value='Play with options and watch the probability space evolve dynamically. Remember, smaller the value of $C$, stronger the regularization strength')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2018b6557ee43009cad6f92bb9251b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(RadioButtons(description='Criterion Type', options=('gini', 'entropy'), value='gini'), FloatSlider(value=0.1, continuous_update=False, description='Regularization ($10^{-C}$), $C$', max=2.0, min=0.1, style=SliderStyle(description_width='initial')), FloatSlider(value=0.1, description='Test fraction ($X_{test}$)', max=0.5, min=0.1, style=SliderStyle(description_width='initial')), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Import mô hình và dữ liệu cần thiết từ thư viện\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Chia dữ liệu huấn luyện và kiểm tra hợp lý\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "# TODO: Huấn luyện mô hình với dữ liệu\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Visualize mô hình vừa được xây dựng với tập dữ liệu kiểm tra\n",
    "from ipywidgets import interact, interactive, IntSlider, Layout\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Biểu diễn biên quyết định của bộ phân lớp SVC.\n",
    "\n",
    "    Tham số\n",
    "    ----------\n",
    "    ax: Đối tượng subplot của matplotlib\n",
    "    clf: Bộ phân lớp\n",
    "    xx: Tọa độ theo trục tung \n",
    "    yy: Tọa độ theo trục hoành\n",
    "    params: Thư viện tham số ứng với hàm contourf\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Tạo ra lưới các điểm để biểu diễn\n",
    "\n",
    "    Tham số\n",
    "    ----------\n",
    "    x: dữ liệu trên trục X\n",
    "    y: dữ liệu trên trục Y\n",
    "    h: kích cỡ một mắt lưới\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "train_pred = np.array(clf.predict(X_train))\n",
    "train_score = clf.score(X_train,y_train)\n",
    "    \n",
    "test_pred = np.array(clf.predict(X_test))\n",
    "test_score = clf.score(X_test,y_test)\n",
    "\n",
    "def decision_tree_fit(criterion, C, test_size):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size,random_state=101)\n",
    "    clf = DecisionTreeClassifier(criterion=criterion)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    train_pred = np.array(clf.predict(X_train))\n",
    "    train_score = clf.score(X_train,y_train)\n",
    "    \n",
    "    test_pred = np.array(clf.predict(X_test))\n",
    "    test_score = clf.score(X_test,y_test)\n",
    "    \n",
    "    \n",
    "    # Cài đặt lưới 1x2 để biểu diễn dữ liệu\n",
    "    fig, sub = plt.subplots(1,2)\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    \n",
    "    titles = ['Training data acc: %.2f' %(train_score), \n",
    "              'Test data acc: %.2f' %(test_score)]\n",
    "    datas = [X_train, X_test]\n",
    "    labels = [y_train, y_test]\n",
    "    \n",
    "    for data, label, title, ax in zip(datas, labels, titles, sub.flatten()):\n",
    "        # biểu diễn mô hình trong không gian 2 chiều với 2 đặc trưng\n",
    "        X0, X1 = data[:, 0], data[:, 1]\n",
    "        xx, yy = make_meshgrid(X0, X1)\n",
    "        plot_contours(ax, clf, xx, yy,\n",
    "                      cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "        ax.scatter(X0, X1, c=label, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xlabel('Sepal length')\n",
    "        ax.set_ylabel('Sepal width')\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(title)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return (train_score,test_score)\n",
    "\n",
    "# import datasets và mô hình DT\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# import công cụ hỗ trợ chia tập dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tải dữ liệu từ datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:2]\n",
    "y = iris.target\n",
    "\n",
    "from ipywidgets import HBox, Label, FloatSlider\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "lb1 = Label(value=\"Play with options and watch the probability space evolve dynamically. \\\n",
    "Remember, smaller the value of $C$, stronger the regularization strength\",fontsize=15)\n",
    "\n",
    "decision_tree_model=interactive(decision_tree_fit,\n",
    "                      criterion=widgets.RadioButtons(options=['gini', 'entropy'],\n",
    "                                                  description = 'Criterion Type'),\n",
    "                      C=FloatSlider(value=0.1,min=0.1,max=2,step=0.1,\n",
    "                                    description='Regularization ($10^{-C}$), $C$', style=style,\n",
    "                                    continuous_update=False),\n",
    "                      test_size=FloatSlider(value=0.1,min=0.1,max=0.5,step=0.1,\n",
    "                                            description = 'Test fraction ($X_{test}$)', style=style))\n",
    "\n",
    "# Hiển thị mô hình với khả năng tương tác\n",
    "display(lb1)\n",
    "display(decision_tree_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Các thuật toán xây dựng cây quyết định cơ bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vấn đề cơ bản của bài toán xây dựng cây quyết định là:\n",
    "- Xác định thuộc tính/điều kiện của mỗi nút\n",
    "- Thứ tự các nút\n",
    "\n",
    "Trong bài này, chúng ta sẽ làm quen với 2 thuật toán cơ bản nhất là **ID3** (Iterative Dichotomiser 3) và **C4.5** (Classification and Regression Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thuật toán ID3 (Quinlan 1986) chọn thuộc tính tốt nhất của tập huấn luyện được làm nút gốc theo tiêu chuẩn cực đại lượng thu hoạch thông tin (Information Gain). \n",
    "\n",
    "### Entropy\n",
    "Entropy dùng để đo độ không chắc chắn (độ mù mờ của thông tin). Nếu ta tập dữ liệu $D$ có $N$ phần tử, thuộc $C$ lớp và số phần tử mỗi lớp là $N_c$ thì entropy của tập dữ liệu $D$ được tính theo công thức:\n",
    "\n",
    "$$ E(D) = - \\sum_{c=1}^{C} \\frac{N_c}{N}\\log (\\frac{N_c}{N}) = - \\sum_{c=1}^{C}p_c\\log(p_c)$$\n",
    "**Bài tập**: Định nghĩa hàm `entropy(freq)` để tính entropy của phân phối xác suất dữ liệu `freq`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy = 2.576425891682003\n"
     ]
    }
   ],
   "source": [
    "# TODO: Để có thể xây dựng được cây quyết định, việc đầu tiên cần làm là tính \n",
    "#       toán entropy cho dữ liệu với một phân phối cho trước (hoặc được tính\n",
    "#       toán thông qua dữ liệu)\n",
    "#       Định nghĩa hàm entropy(freq) tính toán độ mù mờ của dữ liệu với phân \n",
    "#       phối xác suất freq, là tần suất của mỗi lớp c trong bộ dữ liệu D. Hàm trả về số thực là độ đo entropy tương ứng.\n",
    "import numpy as np\n",
    "import math\n",
    "def entropy(freq):\n",
    "    res = 0\n",
    "    for prob in freq:\n",
    "        res += - prob * math.log(prob, 2)\n",
    "    return res\n",
    "\n",
    "freq = np.array([0.2, 0.3, 0.12, 0.18, 0.08, 0.06, 0.06])\n",
    "print(\"Entropy = {}\".format(entropy(freq)))\n",
    "\n",
    "# Kết quả xấp xỉ 1.786"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy hai thuộc tính\n",
    "Khi thuộc tính $x_i$ được chọn làm nút, chia tập $D$ thành $K$ nhánh con $D_1, D_2,...,D_k$, số lượng phần tử trong mỗi nốt con kí hiệu là $m_k$. Độ đo entropy  sau phép chia này được tính:\n",
    "$$ E(D,x_i) = \\sum_{k=1}^{K} \\frac{m_k}{N}E(D_k)= \\sum_{k\\in K}P(k)E(D_k) $$\n",
    "\n",
    "**Bài tập:** Tính độ đo entropy khi có thêm một thuộc tính."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy with iris: 1.0277298129142294\n"
     ]
    }
   ],
   "source": [
    "# TODO: Khi chọn thêm một thuộc tính làm nốt chia, ta phải tính entropy với\n",
    "#       thuộc tính mới để tìm ra thuộc tính chia tốt nhất.\n",
    "#       Định nghĩa hàm _entropy(data, target, target_attr):\n",
    "#       - data (np.array): tập dữ liệu ban đầu\n",
    "#       - target(np.array): tập nhãn tương ứng với dữ liệu\n",
    "#       - target_attr(id): thuộc tính chia cần tính entropy\n",
    "# Gợi ý: Sử dụng lại hàm entropy()\n",
    "\n",
    "def _entropy(data, target, target_attr):\n",
    "    res = 99999999 # lưu kết quả cuối cùng\n",
    "    split_point = None\n",
    "    data_size, attr_size = np.shape(data) # kích thước data\n",
    "    target_attr_val = data[:, target_attr] # mảng chứa giá trị của target_attr\n",
    "    values, counts = np.unique(target_attr_val, return_counts=True) # đếm mỗi giá trị trong mảng target_attr_val\n",
    "    for i in range(len(values) - 1):\n",
    "        D1, D2 = [], [] \n",
    "        split_point_i = (values[i] + values[i + 1]) / 2 \n",
    "        for j in range(data_size):\n",
    "            if target_attr_val[j] < split_point_i:\n",
    "                D1.append(target[j])\n",
    "            else:\n",
    "                D2.append(target[j])\n",
    "        values_1, counts_1 = np.unique(D1, return_counts=True)\n",
    "        values_2, counts_2 = np.unique(D2, return_counts=True)\n",
    "        P1 = counts_1 / sum(counts_1)\n",
    "        P2 = counts_2 / sum(counts_2)\n",
    "        P = [len(D1) / (len(D1) + len(D2)), len(D2) / (len(D1) + len(D2))]\n",
    "        res_i = P[0] * entropy(P1) + P[1] * entropy(P2)\n",
    "        if (res_i < res):\n",
    "            res = res_i\n",
    "            split_point = split_point_i\n",
    "    return res\n",
    "\n",
    "# Tính entropy cho dữ liệu hoa cẩm chướng khi chọn độ dài lá để chia\n",
    "iris_entropy = _entropy(data, target, 0)\n",
    "print(\"Entropy with iris: {}\".format(iris_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Độ thu hoạch thông tin\n",
    "Độ thu hoạch thông tin được tính là độ giảm entropy khi biết thêm một thông tin $x$:\n",
    "$$ Gain(D,x_i) = G(D,x_i)= E(D) - E(D,x_i) $$\n",
    "\n",
    "Thuộc tính nào cho độ mù mờ thông tin (entropy) nhỏ nhất hay có độ thu hoạch thông tin lớn nhất sẽ được chọn làm thuộc tính tại nút.\n",
    "\n",
    "$$ x^* = \\underset{x}{\\arg\\max}G(D,x_i) = \\underset{x}{\\arg\\min}E(D,x_i) $$\n",
    "**Bài tập:** Viết hàm tính độ thu hoạch thông tin khi thử chọn một thuộc tính làm thuộc tính chia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Dựa vào công thức ở trên, định nghĩa hàm gain(data, new_attr) tính độ\n",
    "#       thu hoạch thông tin khi chia nhỏ tập dữ liệu theo thuộc tính mới.\n",
    "def gain(data, target, new_attr):\n",
    "    # Tính entropy của tập dữ liệu\n",
    "    values, counts = np.unique(target, return_counts=True)\n",
    "    p_counts = counts / np.sum(counts)\n",
    "    data_entropy = entropy(p_counts)\n",
    "    \n",
    "    # Tính entropy khi tập dữ liệu bị chia bởi thuộc tính mới\n",
    "    # Khi chọn thuộc tính lần thứ nhất, thuộc tính chia được chọn trước đó có id = -1\n",
    "    data_entropy_divide = _entropy(data, target, new_attr)\n",
    "    \n",
    "    # Tính độ thu hoạch thông tin\n",
    "    info_gain = data_entropy - data_entropy_divide \n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bài tập:** Với tập dữ liệu hoa cẩm chướng ban đầu, chọn ra thuộc tính chia tốt nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best attribute can be used: 2\n"
     ]
    }
   ],
   "source": [
    "# TODO: Dựa vào các hàm đã xây dựng trước đó, chọn ra nút gốc cho cây quyết định\n",
    "#       của hoa cẩm chướng (trả về chỉ số của thuộc tính trong tập dữ liệu)\n",
    "\n",
    "# chọn thuộc tính tốt nhất với thuật toán ID3\n",
    "best_attr = np.argmax([gain(data, target, i) for i in range(np.shape(data)[1])])\n",
    "print(\"Best attribute can be used: {}\".format(best_attr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5\n",
    "Thuật toán C4.5 được đề xuất năm 1993 bởi Quinlan nhằm khắc phục điểm yếu của thuật toán ID3: áp dụng Tỷ lệ thu hoạch thông tin cực đại (Gain Ratio).\n",
    "\n",
    "Tỷ lệ thu hoạch này phạt các thuộc tính có nhiều giá trị bằng cách thêm vào một hạng tử gọi là `thông tin chia` (Split Information), đại lượng này rất nhạy cảm với việc đánh giá tính rộng và đồng nhất khi chia tách dữ liệu theo giá trị thuộc tính:\n",
    "$$ SplitInformation(D,x_i)=-\\sum_{i=1}^{k} \\frac{\\left|D_i\\right|}{\\left|D\\right|} \\log{\\frac{\\left|D_i\\right|}{\\left|D\\right|}}$$\n",
    "`Split Information` thực tế là entropy của tập dữ liệu `D` ứng với thuộc tính chia `x_i`.\n",
    "\n",
    "Khi đó, tỷ lệ thông tin chia được tính bằng cách chia độ thu hoạch thông tin cho thông tin chia.\n",
    "$$ GainRatio(D, x_i) = \\frac{Gain(D,x_i)}{SplitInformation(D,x_i)} $$\n",
    "\n",
    "**Bài tập:** Hoàn thành hàm `split_infor()` tính thông tin chia và hàm `gain_ratio()` để tỷ lệ thu hoạch thông tin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Định nghĩa hai hàm split_infor() và gain_ratio() để cải thiện thuật toán \n",
    "#       ID3 theo ý tưởng của C4.5\n",
    "def split_infor(data, target, new_attr):\n",
    "    new_attr_val = data[:, new_attr] # array of the value of target attribute from data\n",
    "    values, counts = np.unique(new_attr_val, return_counts=True) # count the number of each value\n",
    "    P = counts / np.sum(counts) # probability distribution of value\n",
    "    return entropy(P)\n",
    "\n",
    "def gain_ratio(data, target, new_attr):\n",
    "    return gain(data, target, new_attr) / split_infor(data, target, new_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Bài tập**: Dựa trên việc cải thiện thuật toán ID3, chọn lại nút gốc cho cây quyết định với dữ liệu hoa cẩm chướng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best attribute can be used: 3\n"
     ]
    }
   ],
   "source": [
    "# TODO: Dựa vào các hàm đã xây dựng trước đó, chọn ra nút gốc cho cây quyết định\n",
    "#       của hoa cẩm chướng (trả về chỉ số của thuộc tính trong tập dữ liệu)\n",
    "\n",
    "# chọn thuộc tính tốt nhất với thuật toán C4.5\n",
    "best_attr = np.argmax([gain_ratio(data, target, i) for i in range(np.shape(data)[1])])\n",
    "print(\"Best attribute can be used: {}\".format(best_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
